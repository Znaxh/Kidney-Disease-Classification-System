# Kidney Disease Classification - Backend

Flask-based REST API for kidney disease classification using deep learning with TensorFlow/Keras.

## ğŸ¯ Overview

The backend provides a complete machine learning pipeline for kidney CT scan image classification, including:
- CNN model training with VGG16 transfer learning
- REST API for image prediction
- MLflow experiment tracking
- DVC data version control
- Modular pipeline architecture

## ğŸ—ï¸ Architecture

```
Backend/
â”œâ”€â”€ src/cnnClassifier/              # Main package
â”‚   â”œâ”€â”€ components/                 # ML components
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py      # Data loading and preprocessing
â”‚   â”‚   â”œâ”€â”€ prepare_base_model.py  # VGG16 model preparation
â”‚   â”‚   â”œâ”€â”€ model_training.py      # Training logic
â”‚   â”‚   â””â”€â”€ model_evaluation_mlflow.py # Evaluation with MLflow
â”‚   â”œâ”€â”€ pipeline/                   # Training pipelines
â”‚   â”‚   â”œâ”€â”€ stage_01_data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ stage_02_prepare_base_model.py
â”‚   â”‚   â”œâ”€â”€ stage_03_model_training.py
â”‚   â”‚   â”œâ”€â”€ stage_04_model_evaluation.py
â”‚   â”‚   â””â”€â”€ prediction.py          # Inference pipeline
â”‚   â”œâ”€â”€ config/                     # Configuration management
â”‚   â”‚   â””â”€â”€ configuration.py       # Config loader
â”‚   â”œâ”€â”€ entity/                     # Data classes
â”‚   â”‚   â””â”€â”€ config_entity.py       # Configuration entities
â”‚   â””â”€â”€ utils/                      # Utility functions
â”‚       â””â”€â”€ common.py              # Common utilities
â”œâ”€â”€ config/                         # Configuration files
â”‚   â””â”€â”€ config.yaml                # Main configuration
â”œâ”€â”€ model/                          # Trained models
â”‚   â””â”€â”€ model.h5                   # Trained CNN model
â”œâ”€â”€ templates/                      # HTML templates
â”‚   â””â”€â”€ index.html                 # Web interface
â”œâ”€â”€ app.py                         # Flask application
â”œâ”€â”€ main.py                        # Training pipeline entry point
â”œâ”€â”€ params.yaml                    # Model parameters
â”œâ”€â”€ dvc.yaml                       # DVC pipeline definition
â””â”€â”€ requirements.txt               # Python dependencies
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Create conda environment
conda create -n kidney-classification python=3.8 -y
conda activate kidney-classification

# Install dependencies
pip install -r requirements.txt
```

### 2. Configuration

Update configuration files as needed:
- `config/config.yaml` - Main configuration
- `params.yaml` - Model hyperparameters

### 3. Run Training Pipeline

```bash
# Run complete pipeline
python main.py

# Or run individual stages
python src/cnnClassifier/pipeline/stage_01_data_ingestion.py
python src/cnnClassifier/pipeline/stage_02_prepare_base_model.py
python src/cnnClassifier/pipeline/stage_03_model_training.py
python src/cnnClassifier/pipeline/stage_04_model_evaluation.py
```

### 4. Start API Server

```bash
python app.py
```

The API will be available at `http://localhost:8080`

## ğŸ”Œ API Endpoints

### Health Check
```http
GET /
```
Returns the web interface for testing.

### Image Prediction
```http
POST /predict
Content-Type: application/json

{
    "image": "base64_encoded_image_string"
}
```

**Response:**
```json
[{"image": "Normal"}]  // or "Tumor"
```

### Model Training
```http
GET /train
POST /train
```
Triggers the complete training pipeline.

## ğŸ§  Model Details

### Architecture
- **Base Model**: VGG16 (pre-trained on ImageNet)
- **Transfer Learning**: Fine-tuned for kidney classification
- **Input Size**: 224x224x3 (RGB images)
- **Classes**: 2 (Normal, Tumor)
- **Optimizer**: Adam with learning rate 0.01

### Training Parameters
```yaml
IMAGE_SIZE: [224, 224, 3]
BATCH_SIZE: 16
EPOCHS: 1
CLASSES: 2
WEIGHTS: imagenet
LEARNING_RATE: 0.01
AUGMENTATION: True
```

### Data Augmentation
- Rotation: Â±40 degrees
- Horizontal flip
- Width/Height shift: Â±20%
- Shear: Â±20%
- Zoom: Â±20%

## ğŸ“Š MLflow Integration

### Start MLflow UI
```bash
mlflow ui
```
Access at `http://localhost:5000`

### Remote Tracking (DagHub)
```bash
export MLFLOW_TRACKING_URI=https://dagshub.com/username/repo.mlflow
export MLFLOW_TRACKING_USERNAME=username
export MLFLOW_TRACKING_PASSWORD=token
```

## ğŸ“ˆ DVC Pipeline

### Initialize DVC
```bash
dvc init
```

### Run Pipeline
```bash
dvc repro
```

### View Pipeline DAG
```bash
dvc dag
```

## ğŸ³ Docker Support

### Build Image
```bash
docker build -t kidney-classification-backend .
```

### Run Container
```bash
docker run -p 8080:8080 kidney-classification-backend
```

## ğŸ§ª Testing

### Unit Tests
```bash
python -m pytest tests/
```

### API Testing
```bash
# Test prediction endpoint
curl -X POST http://localhost:8080/predict \
  -H "Content-Type: application/json" \
  -d '{"image": "base64_encoded_image"}'
```

## ğŸ“ Key Files

- **`app.py`**: Flask application with API endpoints
- **`main.py`**: Training pipeline orchestrator
- **`src/cnnClassifier/pipeline/prediction.py`**: Inference pipeline
- **`config/config.yaml`**: Main configuration file
- **`params.yaml`**: Model hyperparameters
- **`dvc.yaml`**: DVC pipeline definition

## ğŸ”§ Configuration

### Model Parameters (`params.yaml`)
```yaml
AUGMENTATION: True
IMAGE_SIZE: [224, 224, 3]
BATCH_SIZE: 16
INCLUDE_TOP: False
EPOCHS: 1
CLASSES: 2
WEIGHTS: imagenet
LEARNING_RATE: 0.01
```

### Pipeline Configuration (`config/config.yaml`)
Contains paths and settings for:
- Data ingestion
- Model preparation
- Training configuration
- Evaluation settings

## ğŸš¨ Troubleshooting

### Common Issues

1. **Memory Error during training**
   - Reduce batch size in `params.yaml`
   - Use smaller image size

2. **Model not found error**
   - Ensure training pipeline completed successfully
   - Check `model/model.h5` exists

3. **Import errors**
   - Verify all dependencies installed: `pip install -r requirements.txt`
   - Check Python path includes project root

## ğŸ“„ Dependencies

Key dependencies:
- **TensorFlow 2.12.0**: Deep learning framework
- **Flask**: Web framework
- **MLflow 2.2.2**: Experiment tracking
- **DVC**: Data version control
- **Flask-Cors**: Cross-origin resource sharing
- **Pandas**: Data manipulation
- **NumPy**: Numerical computing

See `requirements.txt` for complete list.

## ğŸ¤ Contributing

1. Follow PEP 8 style guidelines
2. Add unit tests for new features
3. Update documentation
4. Test API endpoints thoroughly

## ğŸ“ Support

For backend-specific issues, check:
1. Flask application logs
2. MLflow tracking UI
3. DVC pipeline status
4. Model training logs